import sys
import os

PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, PROJECT_ROOT)

import torch
import time
from typing import List, Optional
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, status
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

from transformers import AutoModelForCausalLM, AutoTokenizer

from config import Config
from src.model import HybridProbe


# ============================================================================
#                              PYDANTIC MODELS
# ============================================================================

class PredictionRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""
    question: str = Field(
        ..., 
        min_length=1, 
        max_length=500,
        description="–í–æ–ø—Ä–æ—Å –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ",
        examples=["What is the capital of France?"]
    )
    sentence: str = Field(
        ..., 
        min_length=1, 
        max_length=1000,
        description="–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏",
        examples=["Paris is the capital and most populous city of France."]
    )


class PredictionResponse(BaseModel):
    """–û—Ç–≤–µ—Ç —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º"""
    prediction: int = Field(..., description="–ö–ª–∞—Å—Å: 0=entailment, 1=not_entailment")
    label: str = Field(..., description="–¢–µ–∫—Å—Ç–æ–≤–∞—è –º–µ—Ç–∫–∞ –∫–ª–∞—Å—Å–∞")
    confidence: float = Field(..., ge=0, le=1, description="–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏")
    prob_entailment: float = Field(..., ge=0, le=1, description="P(entailment)")
    prob_not_entailment: float = Field(..., ge=0, le=1, description="P(not_entailment)")
    inference_time_ms: float = Field(..., description="–í—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö")


class BatchRequest(BaseModel):
    """Batch –∑–∞–ø—Ä–æ—Å"""
    items: List[PredictionRequest] = Field(
        ..., 
        max_length=32,
        description="–°–ø–∏—Å–æ–∫ –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ (–º–∞–∫—Å–∏–º—É–º 32)"
    )


class BatchResponse(BaseModel):
    """Batch –æ—Ç–≤–µ—Ç"""
    results: List[PredictionResponse]
    total_time_ms: float
    count: int


class HealthResponse(BaseModel):
    """–°—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞"""
    status: str
    model_loaded: bool
    model_name: str
    probe_parameters: int
    best_accuracy: Optional[float]
    device: str


class ExampleItem(BaseModel):
    """–ü—Ä–∏–º–µ—Ä –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    question: str
    sentence: str
    expected: str


class ExamplesResponse(BaseModel):
    """–°–ø–∏—Å–æ–∫ –ø—Ä–∏–º–µ—Ä–æ–≤"""
    examples: List[ExampleItem]


# ============================================================================
#                              GLOBAL STATE
# ============================================================================

class ModelState:
    """–ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å –º–æ–¥–µ–ª—è–º–∏"""
    def __init__(self):
        self.llm = None
        self.tokenizer = None
        self.probe = None
        self.config = None
        self.best_acc = None
        self.is_loaded = False
        self.device = None


state = ModelState()


# ============================================================================
#                              –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
# ============================================================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ —Å–µ—Ä–≤–µ—Ä–∞.
    –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ.
    """
    print("=" * 60)
    print("–ó–ê–ü–£–°–ö FASTAPI –°–ï–†–í–ï–†–ê")
    print("=" * 60)
    
    state.config = Config()
    state.device = state.config.device
    print(f"Device: {state.device}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –º–æ–¥–µ–ª–∏
    checkpoint_path = os.path.join(PROJECT_ROOT, state.config.save_dir, 'best_model.pt')
    if not os.path.exists(checkpoint_path):
        print(f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {checkpoint_path}")
        print("   –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å: python scripts/train.py")
        # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É, –Ω–æ –º–æ–¥–µ–ª—å –Ω–µ –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–∞
        yield
        return

    print(f"\n –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {state.config.model_name}")
    state.tokenizer = AutoTokenizer.from_pretrained(state.config.model_name)
    if state.tokenizer.pad_token is None:
        state.tokenizer.pad_token = state.tokenizer.eos_token
    print("   ‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ LLM
    print(f"\n –ó–∞–≥—Ä—É–∑–∫–∞ LLM: {state.config.model_name}")
    state.llm = AutoModelForCausalLM.from_pretrained(
        state.config.model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        output_hidden_states=True
    )
    state.llm.eval()
    hidden_dim = state.llm.config.hidden_size
    print(f"   ‚úÖ LLM –∑–∞–≥—Ä—É–∂–µ–Ω (hidden_dim={hidden_dim})")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ Probe
    print(f"\n –°–æ–∑–¥–∞–Ω–∏–µ Probe –º–æ–¥–µ–ª–∏")
    state.probe = HybridProbe(
        hidden_dim=hidden_dim,
        num_extra_layers=len(state.config.extra_layers),
        num_heads=state.config.num_heads,
        num_transformer_layers=state.config.num_transformer_layers,
        ff_dim=state.config.ff_dim,
        max_seq_len=state.config.max_length,
        dropout=state.config.dropout,
        noise_std=state.config.noise_std
    )
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ Probe
    print(f"\n –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location=state.device, weights_only=False)
    state.probe.load_state_dict(checkpoint['model_state_dict'])
    state.best_acc = checkpoint.get('best_acc', None)
    print(f"   ‚úÖ –í–µ—Å–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã (best_acc={state.best_acc:.4f})" if state.best_acc else "   ‚úÖ –í–µ—Å–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    
    state.probe = state.probe.to(state.device)
    state.probe.eval()
    state.is_loaded = True
    
    print("\n" + "=" * 60)
    print(f"‚úÖ –°–ï–†–í–ï–† –ì–û–¢–û–í")
    print(f"   Probe parameters: {state.probe.num_parameters:,}")
    print(f"   API docs: http://localhost:8000/docs")
    print("=" * 60 + "\n")
    
    yield
    
    # –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤
    print("\n –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤...")
    del state.llm
    del state.tokenizer
    del state.probe
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    print("‚úÖ –†–µ—Å—É—Ä—Å—ã –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω—ã")


# ============================================================================
#                              FASTAPI APP
# ============================================================================

app = FastAPI(
    title="QNLI Probe API",
    description="""
    ## API –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ Question-Answering NLI
    
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç –Ω–∞ –∑–∞–¥–∞–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å.
    
    ### –ö–ª–∞—Å—Å—ã:
    - **entailment (0)**: –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—Ç–≤–µ—Ç
    - **not_entailment (1)**: –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ù–ï —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—Ç–≤–µ—Ç
    
    ### Endpoints:
    - `POST /predict` - –æ–¥–∏–Ω–æ—á–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    - `POST /predict/batch` - batch –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (–¥–æ 32 –ø—Ä–∏–º–µ—Ä–æ–≤)
    - `GET /health` - —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞
    - `GET /examples` - –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    """,
    version="1.0.0",
    lifespan=lifespan
)

# —Ä–∞–∑—Ä–µ—à–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã —Å –ª—é–±—ã—Ö –¥–æ–º–µ–Ω–æ–≤
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============================================================================
#                              –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# ============================================================================

def format_input(question: str, sentence: str) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–∞ (–∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)"""
    return (
        f"QNLI Task.\n"
        f"Does the sentence \"{sentence}\" contain the answer "
        f"to the question \"{question}\"?\n"
        f"Answer:"
    )


@torch.no_grad()
def run_inference(question: str, sentence: str) -> dict:
    """
    –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –¥–ª—è –æ–¥–Ω–æ–π –ø–∞—Ä—ã –≤–æ–ø—Ä–æ—Å-–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ.
    
    Returns:
        dict —Å prediction, label, confidence, probabilities
    """
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤—Ö–æ–¥
    text = format_input(question, sentence)
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    inputs = state.tokenizer(
        text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=state.config.max_length
    ).to(state.device)
    
    # –ü–æ–ª—É—á–∞–µ–º hidden states –∏–∑ LLM
    outputs = state.llm(**inputs, output_hidden_states=True)
    attention_mask = inputs["attention_mask"]
    seq_length = attention_mask.sum() - 1
    
    # Main layer ‚Äî –ø–æ–ª–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    main_hidden = outputs.hidden_states[state.config.main_layer]
    real_len = int(attention_mask[0].sum().item())
    main_seq = main_hidden[0, :real_len, :].unsqueeze(0).float()
    main_mask = attention_mask[0, :real_len].unsqueeze(0).float()
    
    # Extra layers ‚Äî pooled (–ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω)
    extra_pooled = []
    for layer in state.config.extra_layers:
        hidden = outputs.hidden_states[layer]
        pooled = hidden[0, seq_length, :].unsqueeze(0).float()
        extra_pooled.append(pooled)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —á–µ—Ä–µ–∑ Probe
    logits = state.probe(main_seq, main_mask, extra_pooled, add_noise=False)
    probs = torch.softmax(logits, dim=-1)
    pred = logits.argmax(dim=-1).item()
    
    return {
        'prediction': pred,
        'label': 'entailment' if pred == 0 else 'not_entailment',
        'confidence': float(probs[0, pred].item()),
        'prob_entailment': float(probs[0, 0].item()),
        'prob_not_entailment': float(probs[0, 1].item())
    }


# ============================================================================
#                              API ENDPOINTS
# ============================================================================

@app.get("/", tags=["Info"])
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π endpoint —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± API"""
    return {
        "message": "üîç QNLI Probe API",
        "description": "API –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ Question-Answering NLI",
        "endpoints": {
            "docs": "/docs",
            "health": "/health",
            "predict": "/predict",
            "batch": "/predict/batch",
            "examples": "/examples"
        }
    }


@app.get("/health", response_model=HealthResponse, tags=["Info"])
async def health_check():
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –∏ –∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö.
    """
    return HealthResponse(
        status="healthy" if state.is_loaded else "model_not_loaded",
        model_loaded=state.is_loaded,
        model_name=state.config.model_name if state.config else "N/A",
        probe_parameters=state.probe.num_parameters if state.probe else 0,
        best_accuracy=state.best_acc,
        device=str(state.device) if state.device else "N/A"
    )


@app.get("/examples", response_model=ExamplesResponse, tags=["Info"])
async def get_examples():
    """
    –ü–æ–ª—É—á–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è API.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –æ–∂–∏–¥–∞–µ–º—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏.
    """
    examples = [
        ExampleItem(
            question="What is the capital of France?",
            sentence="Paris is the capital and most populous city of France.",
            expected="entailment"
        ),
        ExampleItem(
            question="When was Python created?",
            sentence="Python was conceived in the late 1980s by Guido van Rossum.",
            expected="entailment"
        ),
        ExampleItem(
            question="What is the speed of light?",
            sentence="Einstein developed the theory of relativity.",
            expected="not_entailment"
        ),
        ExampleItem(
            question="How many planets are in the solar system?",
            sentence="The weather today is sunny and warm.",
            expected="not_entailment"
        ),
        ExampleItem(
            question="Who wrote Romeo and Juliet?",
            sentence="William Shakespeare was an English playwright and poet.",
            expected="entailment"
        ),
        ExampleItem(
            question="What is machine learning?",
            sentence="Machine learning is a subset of artificial intelligence that enables systems to learn from data.",
            expected="entailment"
        )
    ]
    return ExamplesResponse(examples=examples)


@app.post("/predict", response_model=PredictionResponse, tags=["Prediction"])
async def predict(request: PredictionRequest):
    """
    –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–π –ø–∞—Ä—ã –≤–æ–ø—Ä–æ—Å-–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ.
    
    ### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - **question**: –í–æ–ø—Ä–æ—Å –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ
    - **sentence**: –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –æ—Ç–≤–µ—Ç
    
    ### –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    - **prediction**: –ß–∏—Å–ª–æ–≤–æ–π –∫–ª–∞—Å—Å (0 –∏–ª–∏ 1)
    - **label**: –¢–µ–∫—Å—Ç–æ–≤–∞—è –º–µ—Ç–∫–∞ (entailment / not_entailment)
    - **confidence**: –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (0-1)
    - **prob_entailment**: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞ entailment
    - **prob_not_entailment**: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞ not_entailment
    - **inference_time_ms**: –í—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å
    if not state.is_loaded:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ /health –¥–ª—è –¥–µ—Ç–∞–ª–µ–π."
        )
    
    try:
        start_time = time.time()
        result = run_inference(request.question, request.sentence)
        inference_time_ms = (time.time() - start_time) * 1000
        
        return PredictionResponse(
            **result,
            inference_time_ms=round(inference_time_ms, 2)
        )
    
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"–û—à–∏–±–∫–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: {str(e)}"
        )


@app.post("/predict/batch", response_model=BatchResponse, tags=["Prediction"])
async def predict_batch(request: BatchRequest):
    """
    Batch –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ.
    
    ### –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:
    - –ú–∞–∫—Å–∏–º—É–º 32 –ø—Ä–∏–º–µ—Ä–∞ –∑–∞ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å
    
    ### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - **items**: –°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ —Å –ø–æ–ª—è–º–∏ question –∏ sentence
    
    ### –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    - **results**: –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    - **total_time_ms**: –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    - **count**: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
    """
    if not state.is_loaded:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ /health –¥–ª—è –¥–µ—Ç–∞–ª–µ–π."
        )
    
    start_time = time.time()
    results = []
    
    for item in request.items:
        try:
            item_start = time.time()
            result = run_inference(item.question, item.sentence)
            item_time_ms = (time.time() - item_start) * 1000
            
            results.append(PredictionResponse(
                **result,
                inference_time_ms=round(item_time_ms, 2)
            ))
        except Exception as e:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–∏–º–µ—Ä–∞: {str(e)}"
            )
    
    total_time_ms = (time.time() - start_time) * 1000
    
    return BatchResponse(
        results=results,
        total_time_ms=round(total_time_ms, 2),
        count=len(results)
    )


# ============================================================================
#                              –ó–ê–ü–£–°–ö
# ============================================================================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True
    )