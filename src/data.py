"""
–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ hidden states
"""
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from tqdm import tqdm
from typing import Tuple, List, Dict, Optional
import gc


def format_input(example: dict) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞ QNLI –≤ –ø—Ä–æ–º–ø—Ç"""
    question = example['question']
    sentence = example['sentence']
    return (
        f"QNLI Task.\n"
        f"Does the sentence \"{sentence}\" contain the answer "
        f"to the question \"{question}\"?\n"
        f"Answer:"
    )


def load_model_and_tokenizer(model_name: str) -> Tuple:
    """–ó–∞–≥—Ä—É–∑–∫–∞ LLM –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è hidden states"""
    print(f"\n{'='*60}")
    print("–ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò")
    print("="*60)
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        output_hidden_states=True
    )
    model.eval()
    
    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_name}")
    print(f"   Hidden size: {model.config.hidden_size}")
    print(f"   Num layers: {model.config.num_hidden_layers}")
    
    return model, tokenizer


def load_data(
    dataset_name: str,
    dataset_config: str,
    train_samples: Optional[int] = None
) -> Tuple:
    """–ó–∞–≥—Ä—É–∑–∫–∞ QNLI –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    print(f"\n{'='*60}")
    print("–ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•")
    print("="*60)
    
    dataset = load_dataset(dataset_name, dataset_config)
    
    if train_samples is None:
        train_data = dataset["train"]
    else:
        train_data = dataset["train"].select(
            range(min(train_samples, len(dataset["train"])))
        )
    
    val_data = dataset["validation"]
    
    print(f"‚úÖ Train: {len(train_data)} samples")
    print(f"‚úÖ Val: {len(val_data)} samples")
    
    # –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
    train_labels = [ex["label"] for ex in train_data]
    val_labels = [ex["label"] for ex in val_data]
    
    print(f"\nüìä Train –±–∞–ª–∞–Ω—Å: class0={train_labels.count(0)}, class1={train_labels.count(1)}")
    print(f"üìä Val –±–∞–ª–∞–Ω—Å: class0={val_labels.count(0)}, class1={val_labels.count(1)}")
    
    return train_data, val_data


def extract_hybrid_data(
    model,
    tokenizer,
    texts: List[str],
    main_layer: int,
    extra_layers: List[int],
    max_length: int,
    batch_size: int,
    device: str
) -> Tuple[List, List, Dict]:
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ hidden states –∏–∑ LLM.
    
    Returns:
        main_hidden: —Å–ø–∏—Å–æ–∫ —Ç–µ–Ω–∑–æ—Ä–æ–≤ (seq_len, hidden_dim) –¥–ª—è main_layer
        main_masks: —Å–ø–∏—Å–æ–∫ –º–∞—Å–æ–∫
        extra_pooled: dict {layer: tensor (N, hidden_dim)}
    """
    print(f"\nüì§ –ò–∑–≤–ª–µ–∫–∞–µ–º hidden states...")
    print(f"   Main layer {main_layer}: sequences")
    print(f"   Extra layers {extra_layers}: pooled")
    
    main_hidden = []
    main_masks = []
    extra_pooled = {layer: [] for layer in extra_layers}
    
    model.eval()
    with torch.no_grad():
        for i in tqdm(range(0, len(texts), batch_size), desc="Extracting"):
            batch_texts = texts[i:i + batch_size]
            
            inputs = tokenizer(
                batch_texts,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=max_length
            ).to(device)
            
            outputs = model(**inputs, output_hidden_states=True)
            attention_mask = inputs["attention_mask"]
            
            seq_lengths = attention_mask.sum(dim=1) - 1
            batch_indices = torch.arange(attention_mask.size(0), device=device)
            
            # Main layer sequences
            main = outputs.hidden_states[main_layer]
            
            for j in range(main.size(0)):
                real_len = int(attention_mask[j].sum().item())
                hidden_fp16 = main[j, :real_len, :].cpu().half()
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN/Inf
                if torch.isnan(hidden_fp16).any() or torch.isinf(hidden_fp16).any():
                    print(f"‚ö†Ô∏è NaN/Inf –≤ –ø—Ä–∏–º–µ—Ä–µ {i+j}, –∏—Å–ø–æ–ª—å–∑—É–µ–º float32")
                    hidden_fp16 = main[j, :real_len, :].cpu().float()
                
                main_hidden.append(hidden_fp16)
                main_masks.append(attention_mask[j, :real_len].cpu().half())
            
            # Extra layers pooled (last token)
            for layer in extra_layers:
                hidden = outputs.hidden_states[layer]
                pooled = hidden[batch_indices, seq_lengths]
                extra_pooled[layer].append(pooled.cpu().half())
            
            del outputs, inputs
            torch.cuda.empty_cache()
    
    # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è extra_pooled
    for layer in extra_layers:
        extra_pooled[layer] = torch.cat(extra_pooled[layer], dim=0)
    
    return main_hidden, main_masks, extra_pooled


class HybridDataset(Dataset):
    """
    Dataset –¥–ª—è probe.
    –•—Ä–∞–Ω–∏—Ç –¥–∞–Ω–Ω—ã–µ –≤ float16, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ float32 –ø—Ä–∏ –¥–æ—Å—Ç—É–ø–µ.
    """
    
    def __init__(
        self,
        main_hidden: List[torch.Tensor],
        main_masks: List[torch.Tensor],
        extra_pooled_dict: Dict[int, torch.Tensor],
        labels: torch.Tensor,
        extra_layers: List[int]
    ):
        self.main_hidden = main_hidden
        self.main_masks = main_masks
        self.extra_layers = extra_layers
        self.extra_pooled = [extra_pooled_dict[l] for l in extra_layers]
        self.labels = labels
        
        print(f"HybridDataset: {len(labels)} samples")
    
    def __len__(self) -> int:
        return len(self.labels)
    
    def __getitem__(self, idx: int):
        main_seq = self.main_hidden[idx].float()
        main_mask = self.main_masks[idx].float()
        extra = [self.extra_pooled[i][idx].float() for i in range(len(self.extra_layers))]
        label = self.labels[idx]
        
        return main_seq, main_mask, extra, label


def collate_hybrid(batch: List) -> Tuple:
    """Collate function —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º padding"""
    batch_size = len(batch)
    num_extra = len(batch[0][2])
    
    max_len = max(b[0].size(0) for b in batch)
    hidden_dim = batch[0][0].size(1)
    
    main_padded = torch.zeros(batch_size, max_len, hidden_dim)
    mask_padded = torch.zeros(batch_size, max_len)
    
    for i, b in enumerate(batch):
        seq_len = b[0].size(0)
        main_padded[i, :seq_len, :] = b[0]
        mask_padded[i, :seq_len] = b[1]
    
    extra_pooled = [torch.stack([b[2][j] for b in batch]) for j in range(num_extra)]
    labels = torch.stack([b[3] for b in batch])
    
    return main_padded, mask_padded, extra_pooled, labels


def create_dataloaders(
    train_dataset: HybridDataset,
    val_dataset: HybridDataset,
    batch_size: int
) -> Tuple[DataLoader, DataLoader]:
    """–°–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤"""
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=collate_hybrid,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=collate_hybrid,
        pin_memory=True
    )
    
    return train_loader, val_loader


def cleanup_llm(model) -> None:
    """–û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –æ—Ç LLM"""
    print("\nüßπ –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –æ—Ç LLM...")
    del model
    gc.collect()
    torch.cuda.empty_cache()
    print("‚úÖ –ü–∞–º—è—Ç—å –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∞")