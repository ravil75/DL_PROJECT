# üöÄ HybridProbe: Multi-Layer Fusion –Ω–∞ Qwen2.5-0.5B

## üìä –û–ø–∏—Å–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞

–û–±—É—á–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ probe-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º hidden states –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ—ë–≤ –º–æ–¥–µ–ª–∏ Qwen2.5-0.5B. –í—ã–±–æ—Ä —Å–ª–æ—ë–≤ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ —Å MLPProbe (—Å–ª–æ–∏ 13-14 –ø–æ–∫–∞–∑–∞–ª–∏ –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã).

**–î–∞—Ç–∞—Å–µ—Ç**: 104 743 train / 5 463 validation (–ø–æ–ª–Ω—ã–π QNLI)  
**–ú–æ–¥–µ–ª—å**: Qwen/Qwen2.5-0.5B (896 hidden dim)  
**–ü–æ–¥—Ö–æ–¥**: Transformer Probe + Multi-layer Fusion

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

- **Main layer**: 13 (sequences ‚Üí Transformer encoder)
- **Extra layers**: [9, 10, 11, 12, 14, 15, 16] (pooled ‚Üí weighted fusion)
- **Learnable layer weights**: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–ª–æ—ë–≤
- **Attention pooling**: –∞–≥—Ä–µ–≥–∞—Ü–∏—è sequence ‚Üí vector

### –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
| –ú–µ—Ç–æ–¥ | –ó–Ω–∞—á–µ–Ω–∏–µ |
|-------|----------|
| Dropout | 0.4 |
| Weight Decay | 0.2 |
| Label Smoothing | 0.15 |
| Mixup Alpha | 0.2 |
| R-Drop | ‚úì |

## üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã

| –í—ã–±–æ—Ä–∫–∞ | Accuracy | Precision | Recall | F1-Score |
|---------|----------|-----------|--------|----------|
| **Train** | 91.61% | 0.9161 | 0.9161 | 0.9161 |
| **Validation** | **91.36%** | 0.9137 | 0.9137 | 0.9136 |

**Overfitting gap**: 0.25% ‚úÖ (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π)

### Per-Class Performance (Validation)

| –ö–ª–∞—Å—Å | Precision | Recall | F1-Score | Support |
|-------|-----------|--------|----------|---------|
| Not Entailment | 90.52% | 92.19% | 91.35% | 2 702 |
| Entailment | 92.22% | 90.55% | 91.37% | 2 761 |

### Confidence Analysis
- Mean confidence (correct): **85.70%**
- Mean confidence (wrong): **71.67%**
- High-confidence errors (>0.9): **12**

## üìà –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–≤—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏

| –ú–µ—Ç–æ–¥ | Val Accuracy | Improvement |
|-------|--------------|-------------|
| LLM Zero-Shot | ~58% | ‚Äî |
| MLPProbe (layer 14) | 83.93% | ‚Äî |
| **HybridProbe (multi-layer)** | **91.36%** | **+7.43%** |

## üí° –í—ã–≤–æ–¥—ã

1. **Multi-layer fusion** –¥–∞—ë—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞–¥ single-layer probe (+7.4%)
2. **–°–∏–ª—å–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è** (dropout 0.4, R-Drop, Mixup) –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç overfitting
3. **Balanced performance**: –æ–±–µ –∫–ª–∞—Å—Å—ã –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –æ–¥–∏–Ω–∞–∫–æ–≤–æ —Ö–æ—Ä–æ—à–æ
4. **–•–æ—Ä–æ—à–∞—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞**: –º–æ–¥–µ–ª—å —É–≤–µ—Ä–µ–Ω–∞ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–∞—Ö, –º–µ–Ω–µ–µ —É–≤–µ—Ä–µ–Ω–∞ –≤ –æ—à–∏–±–∫–∞—Ö

## üîß –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã

```python
Config:
    main_layer = 13
    extra_layers = [9, 10, 11, 12, 14, 15, 16]
    num_heads = 4
    num_transformer_layers = 1
    ff_dim = 256
    dropout = 0.4
    learning_rate = 5e-5
    weight_decay = 0.2
    batch_size = 64
    epochs = 50 (early stopping)